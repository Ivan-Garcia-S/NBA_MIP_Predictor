    This project has proved to be a lot tougher than I initially believed it to be.  I was able to train the models to predict the MIP winner for a number of the years correctly, 
but it was very off for other years.  The main problem was that it was hard for the models to find correlation between the feature variables and the MIP Share. 

    I believe predicting the winner of the MIP proved much more difficult than predicting the winnner of the MVP award for a number of reasons. Perhaps the most important is that the
description for who wins the award is a bit ambiguous and open-ended.  From pr.nba.com: "The annual award, which was first presented in the 1985-86 season, is designed to honor an 
up-and-coming player who has made a dramatic improvement from the previous season or seasons." (https://pr.nba.com/ja-morant-2021-22-kia-nba-most-improved-player-award/) This makes 
it very difficult to predict as it's completely up to the voters as to whether they look at a potential candidate's full history in the league, or their most recent season.  Since both 
are possible according to the NBA, both need to be accounted for in the model, but the trick is to figure out when to lean on one over the other.  Here's an example of both scenarios.

    Channing Frye in the 2009-10 season bumped up to 11.2 PPG and 5.3 RPG from 4.2 PPG and 2.2 RPG the year before. However he was tied for 14th in the MIP voting that year. This can be 
attributed to him having averaged 12.3 PPG and 5.8 RPG in his rookie season in 2005-06.  On the other hand, Coby white in 2023-24 season bumped his averages up from the year before 
by 9.4 PPG, 2.3 APG, 1.4 RPG, and won 2nd place in the award rankings by a very small margin to Tyrese Maxey. This is despite the fact that he actually had a much better year in the 
2020-21 season, where he only improved his current season averages by 4.0 PPG, 0.3 APG, and 0.4 RPG compared to that year.  These two examples alone show how it's completely under the 
mercy of the voters as to the weight previous seasons hold. 

    The next piece of the award description that is open-ended is the "dramatic improvement" portion.  There are many ways this can be interpreted.  Usually, it means the player's with the highest 
PPG increase from the previous years will get the most votes, but sometimes that is not the case.  In 2010-11, Dorell Wright finished 3rd in MIP voting behind Kevin Love and Lamarcus Aldridge,
despite having a huge 130% increase in his PPG average (7.1 -> 16.4). This production also helped the Warriors to win 10 more games than the season before.  Still, Love won the award with a 
29% increase in PPG average (20.2 -> 26.0). What did stand out for Love was his impressive 15.2 RPG that season, bumped from 11.0 RPG the prior year. Since the year 2000, only 5 players have 
managed to reach the 15 RPG mark for an entire season, making him a well deserved MIP winner.  On the other hand, Minnesota had a terrible performance that year, with a record of 17-65.  

    This brings me to my next point, of how team performance seems to have high priority in voter's minds some years and completely overlooked other years.  When Kevin Love won in 2010-11, the 
TWolves had the worst record in the league with only 17 wins â€“ indicating that record is irrelevant.  On the other hand in the 2016-17 season, Nikola Jokic and Giannis Antetokounmpo were the 
two clear favorites for the award. Although Jokic had more of an increase to his stats all around, the award went to Giannis that year.  Jokic increased his PPG by 6.7 pts, doubled his APG from 2.4 
to 4.9, and added almost 3 more RPG going from 7.0 to 9.8.  Giannis, on the other hand, added 6.0 PPG, 1.7 APG, and 1.0 APG to his averages from the previous year.  The major difference is that 
Giannis led the Bucks to the 6th seed in 2017 after coming off a year where they sat at 12th and missed the playoffs. Meanwhile, Denver did improve from the year before but still missed the playoffs,
moving from the 11 seed to the 9 seed in the West.

    With all this said, it makes sense why MIP winners are much harder to predict purely through numbers, as opposed to the MVP.  To determine the MVP winner, you can measure their value based on their team
and individual performance from that same year, whereas with MIP awards it can be a combination of any number of past seasons of team and individual difference in performance.  This large increase
in variability makes it extremely tough to train a model that can predict the MIP winner over the course of many seasons.

    (Speak on all the feature variables I tried to include in model)

    

